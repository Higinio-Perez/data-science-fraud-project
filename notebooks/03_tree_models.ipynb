{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4d20e6e",
   "metadata": {},
   "source": [
    "# Tree Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea323ca",
   "metadata": {},
   "source": [
    "### Imports and data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eeff027b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"../data/transactions.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10c9a4b",
   "metadata": {},
   "source": [
    "### Feature selection and target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "882db727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same feature set used in the logistic regression baseline\n",
    "# Keeping features consistent allows fair comparison between models\n",
    "features = [\n",
    "    \"amount\",          # Transaction amount\n",
    "    \"night\",           # Night-time indicator\n",
    "    \"weekend\",         # Weekend indicator\n",
    "    \"country_change\",  # Cross-border transaction\n",
    "    \"velocity\",        # Transaction activity intensity\n",
    "    \"device_risk\",     # Device-related risk proxy\n",
    "]\n",
    "\n",
    "# Feature matrix and target vector\n",
    "X = df[features]\n",
    "y = df[\"is_fraud\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8878e1",
   "metadata": {},
   "source": [
    "### Train–test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bfcb802b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets\n",
    "# Stratification preserves the fraud rate in both subsets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.25,     # 25% for evaluation\n",
    "    stratify=y,         # Essential for imbalanced datasets\n",
    "    random_state=42,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2822f55",
   "metadata": {},
   "source": [
    "### Random Forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fc735950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest ROC-AUC: 0.731\n",
      "Random Forest PR-AUC:  0.046\n"
     ]
    }
   ],
   "source": [
    "# Initialize Random Forest classifier\n",
    "# This model captures non-linear relationships and feature interactions\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=300,       # Number of trees\n",
    "    max_depth=12,           # Maximum tree depth (controls overfitting)\n",
    "    min_samples_leaf=50,    # Minimum samples per leaf (regularization)\n",
    "    class_weight=\"balanced\",# Compensate for class imbalance\n",
    "    random_state=42,\n",
    "    n_jobs=-1,              # Use all available CPU cores\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Predict fraud probabilities on the test set\n",
    "rf_prob = rf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluate ranking performance\n",
    "rf_roc = roc_auc_score(y_test, rf_prob)\n",
    "rf_pr = average_precision_score(y_test, rf_prob)\n",
    "\n",
    "print(f\"Random Forest ROC-AUC: {rf_roc:.3f}\")\n",
    "print(f\"Random Forest PR-AUC:  {rf_pr:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2a307e",
   "metadata": {},
   "source": [
    "### Gradient Boosting model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d7e0e993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting ROC-AUC: 0.745\n",
      "Gradient Boosting PR-AUC:  0.050\n"
     ]
    }
   ],
   "source": [
    "# Initialize Gradient Boosting classifier\n",
    "# This model builds trees sequentially, each correcting previous errors\n",
    "gb = GradientBoostingClassifier(\n",
    "    n_estimators=200,    # Number of boosting stages\n",
    "    learning_rate=0.05,  # Contribution of each tree\n",
    "    max_depth=3,         # Shallow trees (weak learners)\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "gb.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Predict fraud probabilities on the test set\n",
    "gb_prob = gb.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluate ranking performance\n",
    "gb_roc = roc_auc_score(y_test, gb_prob)\n",
    "gb_pr = average_precision_score(y_test, gb_prob)\n",
    "\n",
    "print(f\"Gradient Boosting ROC-AUC: {gb_roc:.3f}\")\n",
    "print(f\"Gradient Boosting PR-AUC:  {gb_pr:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f186cf06",
   "metadata": {},
   "source": [
    "### Recalculate Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5c44c251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Logistic ROC-AUC: 0.748\n",
      "Baseline Logistic PR-AUC:  0.051\n"
     ]
    }
   ],
   "source": [
    "# Scale features for logistic regression (important for stable optimization)\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)  \n",
    "X_test_scaled = scaler.transform(X_test)        \n",
    "\n",
    "# Train logistic regression baseline (with class balancing for rare fraud cases)\n",
    "log_model = LogisticRegression(\n",
    "    max_iter=1000,             # allow enough iterations to converge\n",
    "    class_weight=\"balanced\",   # counter class imbalance\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "# Fit the model on the scaled training data\n",
    "log_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict probabilities for the positive class (fraud = 1)\n",
    "y_prob = log_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# Quick sanity check metrics (optional but useful)\n",
    "logistic_roc = roc_auc_score(y_test, y_prob)\n",
    "logistic_pr = average_precision_score(y_test, y_prob)\n",
    "\n",
    "print(f\"Baseline Logistic ROC-AUC: {logistic_roc:.3f}\")\n",
    "print(f\"Baseline Logistic PR-AUC:  {logistic_pr:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff232202",
   "metadata": {},
   "source": [
    "### Models Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dc01438c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>ROC-AUC</th>\n",
       "      <th>PR-AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.747905</td>\n",
       "      <td>0.051441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.730679</td>\n",
       "      <td>0.045580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>0.744959</td>\n",
       "      <td>0.049701</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Model   ROC-AUC    PR-AUC\n",
       "0  Logistic Regression  0.747905  0.051441\n",
       "1        Random Forest  0.730679  0.045580\n",
       "2    Gradient Boosting  0.744959  0.049701"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compare all models using the same evaluation metrics\n",
    "results = pd.DataFrame(\n",
    "    {\n",
    "        \"Model\": [\n",
    "            \"Logistic Regression\",\n",
    "            \"Random Forest\",\n",
    "            \"Gradient Boosting\",\n",
    "        ],\n",
    "        \"ROC-AUC\": [\n",
    "            logistic_roc,\n",
    "            rf_roc,\n",
    "            gb_roc,\n",
    "        ],\n",
    "        \"PR-AUC\": [\n",
    "            logistic_pr,\n",
    "            rf_pr,\n",
    "            gb_pr,\n",
    "        ],\n",
    "    }\n",
    ")\n",
    "\n",
    "results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5618ef",
   "metadata": {},
   "source": [
    "## Tree-Based Models Summary \n",
    "\n",
    "After establishing an interpretable baseline with logistic regression, we evaluate more expressive tree-based models to improve fraud detection performance.\n",
    "Fraud detection is a rare-event classification problem (highly imbalanced), so the main goal is to improve the model’s ability to *rank* truly fraudulent transactions above legitimate ones.\n",
    "\n",
    "To ensure a fair comparison, all models are evaluated using the same train–test split and the same feature set.\n",
    "\n",
    "---\n",
    "\n",
    "### Why tree-based models?\n",
    "\n",
    "Logistic regression is a strong baseline because it is fast and interpretable, but it is fundamentally a linear model.\n",
    "Tree-based methods can capture:\n",
    "- **Non-linear relationships** (risk does not always increase linearly with a feature)\n",
    "- **Feature interactions** (e.g., high amount *and* cross-border *and* ecommerce may be much riskier than each feature alone)\n",
    "\n",
    "These properties often lead to improved performance in fraud detection tasks.\n",
    "\n",
    "---\n",
    "\n",
    "### Random Forest (simple intuition)\n",
    "\n",
    "A Random Forest is an ensemble of many decision trees trained independently.\n",
    "Each tree learns a set of “if–then” rules (splits) from the data, and the forest combines all trees to produce a more stable prediction.\n",
    "\n",
    "Key idea:\n",
    "- Instead of trusting one single tree (which can overfit), we train **many trees** and **average their predictions**.\n",
    "- Trees are diversified by training on different random samples of data and different subsets of features.\n",
    "\n",
    "As a result, Random Forests are typically robust and perform well out of the box, especially when the relationship between features and fraud risk is not purely linear.\n",
    "\n",
    "---\n",
    "\n",
    "### Gradient Boosting (simple intuition)\n",
    "\n",
    "Gradient Boosting is also an ensemble of decision trees, but unlike Random Forest, trees are built **sequentially**.\n",
    "Each new tree is trained to **correct the errors** made by the previous ensemble.\n",
    "\n",
    "Key idea:\n",
    "- The model starts simple and improves step-by-step.\n",
    "- Each additional tree focuses on examples that were previously misclassified or poorly ranked.\n",
    "\n",
    "Because it learns iteratively and can capture subtle patterns, Gradient Boosting often provides stronger ranking performance than Random Forest in fraud detection settings (when properly configured).\n",
    "\n",
    "---\n",
    "\n",
    "### Evaluation metrics (fraud-appropriate)\n",
    "\n",
    "Because fraud is rare, overall accuracy is misleading and is not used to judge model quality.\n",
    "Instead, we evaluate models using:\n",
    "\n",
    "- **ROC-AUC**: measures how well the model ranks fraud above non-fraud across many thresholds.\n",
    "- **PR-AUC (Average Precision)**: emphasizes the precision–recall trade-off and is more informative under strong class imbalance.\n",
    "\n",
    "PR-AUC is particularly important in fraud detection because it reflects the quality of the alert list produced by the model.\n",
    "\n",
    "---\n",
    "\n",
    "### Models Comparison\n",
    "\n",
    "The obtained results are consistent with expectations for a highly imbalanced fraud detection task.\n",
    "Despite its simplicity, logistic regression achieves competitive ROC-AUC and the highest PR-AUC among the evaluated models.\n",
    "This suggests that the current feature set already captures most of the available signal in a largely additive manner.\n",
    "Tree-based models provide comparable performance but do not significantly outperform the linear baseline at this stage.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Conclusion and next step\n",
    "\n",
    "Tree-based models typically improve fraud ranking performance compared to the linear baseline.\n",
    "Among the evaluated methods, **Gradient Boosting often provides the strongest overall ranking quality**, making it a good candidate for downstream operational tuning.\n",
    "\n",
    "**Next step:** optimize the decision threshold (precision–recall trade-off) to select an operational point that balances fraud capture (recall) and false alarms (precision), according to practical constraints.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
